---
title: "TESI NLP: Sentence Embedding Analyses"
author: "Chase Antonacci"
date: "2025-10-10"
output:
  html_document:
  toc: true
toc_float: true
theme: united
code_folding: hide
---

## Setup

Load all the necessary R packages for the analysis and set global options.

```{r setup, include=FALSE}
# set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# load packages
library(tidyverse)
library(glmnet)
library(caret)
library(mice)
library(writexl)
library(readxl)
library(naniar)
library(VIM)
library(ggpubr)
library(patchwork)
library(ggrepel)
library(corrplot)
library(nnet)
library(reshape2)
library(xgboost)
library(Matrix)
library(dplyr)
library(data.table)
library(here)
library(rsq)
library(mixOmics)
library(psych)
library(pheatmap)
library(RColorBrewer)
library(viridis)
library(patchwork)
library(ComplexHeatmap)
library(circlize)
library(foreach)
library(doParallel)
library(colorspace)
library(ggrepel)
library(ggeffects)
library(uwot)
library(dbscan)
library(plotly)
library(gghalves)
library(ggplot2)
```

Load data

```{r load-data}
# define the path to your data file
file_path_roberta <- here::here("data", "roberta_embeddings.csv")
file_path_dx      <- here::here("data", "any_intDx_T1-TA.xlsx")

# read in data
df.roberta_imputed <- read.csv(file_path_roberta)
df.dx              <- read_xlsx(file_path_dx)

# set appropriate variables as factors
df.roberta_imputed$demo_Race.T1 <- as.factor(df.roberta_imputed$demo_Race.T1)
df.roberta_imputed$demo_Sex.T1  <- as.factor(df.roberta_imputed$demo_Sex.T1)
```

## Elastic Net - CV Performance

```{r elastic-net-cv}
# ----------------------------
# Run Elastic Net Model with 5-fold Nested CV
# ----------------------------
set.seed(300)

# set up data
y <- df.roberta_imputed$YSR_Internalizing_Total.T3 # DV
X <- as.matrix(df.roberta_imputed[, c(2:6, 8, 12:ncol(df.roberta_imputed))])
X <- matrix(as.numeric(X), nrow = nrow(X), dimnames = dimnames(X))
X <- scale(X)

# outer folds for nested CV
n_outer_folds <- 5
folds <- sample(rep(1:n_outer_folds, length.out = length(y)))

# grid of alpha values for tuning
alpha_grid <- seq(0, 1, by = 0.05)

# initialize performance metrics storage
cv_mse         <- c()
cv_rmse        <- c()
cv_mae         <- c()
cv_r_squared   <- c()
cv_predictions <- rep(NA, length(y))

# store best alpha and lambda per outer fold
best_alpha_folds  <- c()
best_lambda_folds <- c()

# outer CV loop
for (k in 1:n_outer_folds) {
  # split into training and test sets
  train_idx <- which(folds != k)
  test_idx  <- which(folds == k)

  X_train <- X[train_idx, ]
  y_train <- y[train_idx]
  X_test  <- X[test_idx, ]
  y_test  <- y[test_idx]

  # inner loop: tune alpha and lambda
  inner_cv_results <- data.frame()

  for (alpha_val in alpha_grid) {
    cv_model <- cv.glmnet(X_train, y_train, alpha = alpha_val, nfolds = 5)
    inner_cv_results <- rbind(
      inner_cv_results,
      data.frame(
        alpha = alpha_val,
        lambda_min = cv_model$lambda.min,
        cv_mse = min(cv_model$cvm)
      )
    )
  }

  # select best alpha/lambda from inner CV
  best_idx     <- which.min(inner_cv_results$cv_mse)
  best_alpha   <- inner_cv_results$alpha[best_idx]
  best_lambda  <- inner_cv_results$lambda_min[best_idx]

  best_alpha_folds  <- c(best_alpha_folds, best_alpha)
  best_lambda_folds <- c(best_lambda_folds, best_lambda)

  # fit model on outer train data
  final_model <- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda)

  # predict on outer test set
  y_pred <- predict(final_model, X_test, s = best_lambda)
  cv_predictions[test_idx] <- y_pred

  # compute performance metrics for this fold
  mse       <- mean((y_test - y_pred)^2)
  rmse      <- sqrt(mse)
  mae       <- mean(abs(y_test - y_pred))
  r_squared <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

  cv_mse         <- c(cv_mse, mse)
  cv_rmse        <- c(cv_rmse, rmse)
  cv_mae         <- c(cv_mae, mae)
  cv_r_squared   <- c(cv_r_squared, r_squared)
}

# aggregate performance metrics
global_r_squared <- 1 - sum((y - cv_predictions)^2) / sum((y - mean(y))^2)
# R-squared (0.156) reported in text
```

## Elastic Net - Full Model and Coefficient Export

This chunk trains the final model on all data to get the stable coefficient vector needed for the dot product projection.

```{r elastic-net-export-coefs}
# ----------------------------
# Model trained on full data for interpretation
# ----------------------------

set.seed(123)

# set up data (same as CV)
y <- df.roberta_imputed$YSR_Internalizing_Total.T3
X <- as.matrix(df.roberta_imputed[, c(2:6, 8, 12:ncol(df.roberta_imputed))])
X <- matrix(as.numeric(X), nrow = nrow(X), dimnames = dimnames(X))
X <- scale(X)

alpha_grid <- seq(0, 1, by = 0.05)
cv_results <- data.frame()

# tune hyperparameters on full dataset
for (alpha_val in alpha_grid) {
  cv_model <- cv.glmnet(X, y, alpha = alpha_val, nfolds = 5)
  cv_results <- rbind(
    cv_results,
    data.frame(
      alpha = alpha_val,
      lambda_min = cv_model$lambda.min,
      cv_mse = min(cv_model$cvm)
    )
  )
}

# select best alpha and lambda
best_idx     <- which.min(cv_results$cv_mse)
best_alpha   <- cv_results$alpha[best_idx]
best_lambda  <- cv_results$lambda_min[best_idx]

# refit final model on full dataset
final_model <- glmnet(X, y, alpha = best_alpha, lambda = best_lambda)

# extract coefficients
coef_final <- coef(final_model, s = best_lambda)

# ----------------------------
# Save embedding coefficients for Python dot product
# ----------------------------

# extract the full coefficient matrix
coef_matrix <- coef(final_model, s = best_lambda)

# get all coefficient names
coef_names <- rownames(coef_matrix)

# find the indices for only the 768 embedding features
emb_indices <- which(grepl("^emb_", coef_names))

# extract just those 768 coefficients
embedding_coefs <- coef_matrix[emb_indices, , drop = FALSE]

# save as a simple CSV with feature names
write.csv(
  as.matrix(embedding_coefs),
  file = here::here("data", "roberta_embedding_coefficients.csv"),
  row.names = TRUE
)
```

## Elastic Net - Performance Plot

```{r elastic-net-plot, fig.width=6, fig.height=6}
# ----------------------------
# Plotting: Predicted vs Observed Scatterplot
# ----------------------------

plot_data <- data.frame(Actual = y, Predicted = cv_predictions)
r_squared_val <- 1 - sum((plot_data$Actual - plot_data$Predicted)^2) / 
                       sum((plot_data$Actual - mean(plot_data$Actual))^2)
r2_label <- paste0("italic(R)^2 == ", round(r_squared_val, 3))

axis_min <- min(c(plot_data$Actual, plot_data$Predicted), na.rm = TRUE)
axis_max <- max(c(plot_data$Actual, plot_data$Predicted), na.rm = TRUE)

ggplot(plot_data, aes(x = Predicted, y = Actual)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  geom_point(color = viridis(1, option = "D", begin = 0.35), alpha = 0.6) +
  geom_smooth(method = "lm", color = "#FDE725FE", se = TRUE, fill = "gray80") +
  annotate(
    "text",
    x = axis_min + (axis_max - axis_min) * 0.05,
    y = axis_max,
    label = r2_label,
    hjust = 0,
    vjust = 1,
    parse = TRUE,
    size = 5
  ) +
  labs(
    title = "Model Performance: Contextualized Sentence Embeddings",
    subtitle = "Predicted vs. Actual YSR Internalizing Scores from RoBERTa Features",
    x = "Predicted Score (Out-of-Sample)",
    y = "Observed Score"
  ) +
  coord_fixed(ratio = 1, xlim = c(axis_min, axis_max), ylim = c(axis_min, axis_max)) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray30")
  )

ggsave(
  here::here("figures", "RoBERTa_en_predicted_vs_observed.pdf"),
  width = 6, height = 6, units = "in"
)
```

## UMAP Dimensionality Reduction and Clustering

```{r umap-analysis}
# select only embedding columns for UMAP
df.roberta_imputed_umap <- df.roberta_imputed[, 12:ncol(df.roberta_imputed)]
rownames(df.roberta_imputed_umap) <- df.roberta_imputed$ELS_ID
embedding_matrix <- df.roberta_imputed_umap %>%
  dplyr::select(starts_with("emb_")) %>%
  as.matrix()

# ----------------------------
# 2D UMAP
# ----------------------------
set.seed(123)
umap_results <- umap(embedding_matrix, n_components = 2, n_neighbors = 15, min_dist = 0.1)

# run hdbscan on umap space
clusters <- hdbscan(umap_results, minPts = 24)

# create visualization dataframe
umap_plot_data <- tibble(
  ELS_ID = df.roberta_imputed$ELS_ID,
  UMAP1 = umap_results[, 1],
  UMAP2 = umap_results[, 2],
  Cluster = as.factor(clusters$cluster) # Cluster 0 is "noise"
)

# merge cluster assignments back into main dataframe
df_final_with_clusters <- df.roberta_imputed %>%
  left_join(dplyr::select(umap_plot_data, ELS_ID, Cluster, UMAP1, UMAP2), by = "ELS_ID")
```

```{r umap-plot-scatter, fig.width=6.1, fig.height=5}
# ----------------------------
# UMAP Plot (Figure 5c)
# ----------------------------

# define custom color palette
cluster_colors <- c(
  "0" = "gray70",
  "1" = "#440154FF", # deep viridis purple
  "2" = "#21908CFF"  # bright viridis teal
)

ggplot(umap_plot_data, aes(x = UMAP1, y = UMAP2, color = Cluster)) +
  geom_point(alpha = 0.8, size = 2.5) +
  scale_color_manual(
    name = "Linguistic Style",
    values = cluster_colors,
    labels = c(
      "0" = "Unassigned",
      "1" = "Style A (Low Risk)",
      "2" = "Style B (High Risk)"
    )
  ) +
  labs(
    title = "Semantic Map of Linguistic Styles",
    subtitle = "UMAP projection revealing two distinct, clinically relevant clusters",
    x = "UMAP Dimension 1",
    y = "UMAP Dimension 2"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray40", size = 11, margin = margin(b = 10)),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  ) +
  guides(color = guide_legend(override.aes = list(alpha = 1, size = 5)))

ggsave(
  here::here("figures", "RoBERTa_en_UMAP.pdf"),
  width = 6.1, height = 5, units = "in"
)
```

```{r umap-plot-violin, fig.width=7, fig.height=5}
# ----------------------------
# Cluster Violin Plot (Figure 5d)
# ----------------------------

# filter out noise points and create labels
plot_data_filtered <- df_final_with_clusters %>%
  filter(Cluster != 0) %>%
  mutate(
    Cluster_Label = factor(
      Cluster,
      levels = c("1", "2"),
      labels = c("Style A (Low Risk)", "Style B (High Risk)")
    )
  )

# define matching colors
cluster_colors <- c(
  "Style A (Low Risk)" = "#440154FF",
  "Style B (High Risk)" = "#21908CFF"
)

ggplot(
  plot_data_filtered,
  aes(x = Cluster_Label, y = YSR_Internalizing_Total.T3, fill = Cluster_Label)
) +
  geom_jitter(shape = 21, width = 0.15, alpha = 0.6, size = 2) +
  geom_violin(alpha = 0.4, width = 0.6, color = NA) +
  geom_boxplot(width = 0.15, outlier.shape = NA, alpha = 0.8) +
  stat_compare_means(
    comparisons = list(c("Style A (Low Risk)", "Style B (High Risk)")),
    method = "t.test",
    label = "p.signif",
    symnum.args = list(
      cutpoints = c(0, 0.001, 0.01, 0.05, 1),
      symbols = c("***", "**", "*", "ns")
    )
  ) +
  scale_fill_manual(values = cluster_colors) +
  labs(
    title = "Internalizing Problems Differ by Linguistic Style",
    subtitle = "YSR scores for the two primary semantic clusters identified by UMAP",
    x = "Linguistic Style Cluster",
    y = "YSR Internalizing Score"
  ) +
  coord_cartesian(ylim = c(0, NA)) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray40", size = 11, margin = margin(b = 10)),
    legend.position = "none"
  )

ggsave(
  here::here("figures", "RoBERTa_en_UMAP_cluster.pdf"),
  width = 7, height = 5, units = "in"
)
```

## UMAP Models - Linear

```{r umap-models-linear}
# ----------------------------
# Bivariate Correlations and T-Test
# ----------------------------
cor.test(umap_plot_data$UMAP1, df_final_with_clusters$YSR_Internalizing_Total.T3)
# r = .316, p < .001

cor.test(umap_plot_data$UMAP2, df_final_with_clusters$YSR_Internalizing_Total.T3)
# r = .300, p < .001 

t.test(
  df_final_with_clusters[which(df_final_with_clusters$Cluster == 1), "YSR_Internalizing_Total.T3"],
  df_final_with_clusters[which(df_final_with_clusters$Cluster == 2), "YSR_Internalizing_Total.T3"]
)
# t = -4.433, p < .001 

# check correlation between UMAP1 and UMAP2
cor.test(umap_plot_data$UMAP1, umap_plot_data$UMAP2)
# r = .958 


# ----------------------------
# Linear Model Comparison
# ----------------------------
model_umap_without <- lm(
  YSR_Internalizing_Total.T3 ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1,
  data = df_final_with_clusters
)

model_umap_with <- lm(
  YSR_Internalizing_Total.T3 ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1 +
    UMAP1,
  data = df_final_with_clusters
)
# summary(model_umap_with)
# B = 0.435, SE = 0.180, p = .017 

anova(model_umap_without, model_umap_with)
# F(1, 192) = 5.838, p = .017 
```

## UMAP Models - Logistic Regression

```{r umap-models-logistic}
# merge in diagnoses
df.roberta_imputed <- merge(
  df.roberta_imputed,
  df.dx[, c("ELS_ID", "T3_T4_any_intDx")], # select only needed dx col
  by = "ELS_ID",
  all.x = TRUE
)

# specify logistic model
model.logistic.umap <- glm(
  T3_T4_any_intDx ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1 +
    UMAP1,
  data = df_final_with_clusters,
  family = binomial
)

# get model summary
summary(model.logistic.umap)
# p = .023 for UMAP1, p = .825 for SumSev 

# get odds ratios for interpretation
odds_ratios <- exp(coef(model.logistic.umap))
conf_intervals_or <- exp(confint(model.logistic.umap))

results_table <- data.frame(
  Odds_Ratio = odds_ratios,
  CI_Lower = conf_intervals_or[, 1],
  CI_Upper = conf_intervals_or[, 2]
)
# results_table
# OR = 1.151, 95% CI [1.023, 1.303] 
```