---
title: "TESI NLP: LIWC Analyses"
author: "Chase Antonacci"
date: "2025-10-10"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
---

## Setup

Load all necessary R packages for the analysis and set global options.

```{r setup, include=FALSE}
# set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# load packages
library(tidyverse)
library(glmnet)
library(caret)
library(mice)
library(writexl)
library(readxl)
library(naniar)
library(VIM)
library(ggpubr)
library(patchwork)
library(ggrepel)
library(corrplot)
library(nnet)
library(reshape2)
library(xgboost)
library(Matrix)
library(dplyr)
library(data.table)
library(here)
library(rsq)
library(mixOmics)
library(psych)
library(pheatmap)
library(RColorBrewer)
library(viridis)
library(patchwork)
library(ComplexHeatmap)
library(circlize)
library(foreach)
library(doParallel)
library(colorspace)
library(ggrepel)
library(ggeffects)
```

Load data

```{r load-data}
# define file paths using here() for relative paths
path_liwc    <- here::here("data", "LIWC_Imputed_ChildOnly_Final_T1.xlsx")
path_dx      <- here::here("data", "any_intDx_T1-TA.xlsx")

# read in data
df.liwc_imputed <- read_xlsx(path_liwc)
df.dx           <- read_xlsx(path_dx)

# set appropriate variables as factors
df.liwc_imputed$demo_Race.T1 <- as.factor(df.liwc_imputed$demo_Race.T1)
df.liwc_imputed$demo_Sex.T1  <- as.factor(df.liwc_imputed$demo_Sex.T1)
```

## Descriptive figure of LIWC data

```{r liwc-heatmap}
# define LIWC categories/themes from manual
theme_list_curated <- list(
  Summary = c("WC", "Analytic", "Clout", "Authentic", "Tone", "WPS", "BigWords", "Dic"),
  FunctionWords = c(
    "Linguistic", "function", "det", "article", "number", "prep", "auxverb", "adverb",
    "conj", "negate", "verb", "adj", "quantity", "pronoun", "ppron", "i", "we", "you",
    "shehe", "they", "ipron"
  ),
  Drives = c("Drives", "affiliation", "achieve", "power"),
  Cognition = c(
    "Cognition", "allnone", "cogproc", "insight", "cause", "discrep", "tentat",
    "certitude", "differ", "memory"
  ),
  Affect = c(
    "Affect", "tone_pos", "tone_neg", "emotion", "emo_pos", "emo_neg", "emo_anx",
    "emo_anger", "emo_sad", "swear"
  ),
  Social = c(
    "Social", "socbehav", "prosocial", "polite", "conflict", "moral", "comm",
    "socrefs", "family", "friend", "female", "male"
  ),
  Culture = c("Culture", "politic", "ethnicity", "tech"),
  Lifestyle = c("Lifestyle", "leisure", "home", "work", "money", "relig"),
  Physical = c(
    "Physical", "health", "illness", "wellness", "mental", "substances",
    "sexual", "food", "death"
  ),
  States = c("need", "want", "acquire", "lack", "fulfill", "fatigue"),
  Motives = c("reward", "risk", "curiosity", "allure"),
  Perception = c("Perception", "attention", "motion", "space", "visual", "auditory", "feeling"),
  TimeOrientation = c("time", "focuspast", "focuspresent", "focusfuture"),
  Conversational = c("Conversation", "netspeak", "assent", "nonflu", "filler"),
  Punctuation = c("AllPunc", "Period", "Comma", "QMark", "Exclam", "Apostro", "OtherP")
)

# create a vector of all LIWC variable names to be used
ordered_vars_curated <- unlist(theme_list_curated, use.names = FALSE)

# --- Step 2: Prepare Data for Clustering ---
df_full <- df.liwc_imputed %>%
  mutate(ELS_ID = as.character(ELS_ID))

# create the matrix for clustering
data_for_clustering <- df_full %>%
  dplyr::select(all_of(ordered_vars_curated))

# set the row names to be subject IDs
rownames(data_for_clustering) <- df_full$ELS_ID

# scale the data (row names are preserved)
data_scaled <- scale(data_for_clustering)

# --- Step 3: Create Annotations and Colors ---
annotation_col <- data.frame(
  Theme = rep(names(theme_list_curated), times = sapply(theme_list_curated, length))
)
rownames(annotation_col) <- colnames(data_for_clustering)

theme_colors <- list(
  Theme = c(
    "Summary"         = "#595959",
    "FunctionWords"   = "#1f77b4",
    "Drives"          = "#ff7f0e",
    "Cognition"       = "#2ca02c",
    "Affect"          = "#d62728",
    "Social"          = "#9467bd",
    "Culture"         = "#8c564b",
    "Lifestyle"       = "#98df8a",
    "Physical"        = "#e377c2",
    "States"          = "#bcbd22",
    "Motives"         = "#fdbf6f",
    "Perception"      = "#17becf",
    "TimeOrientation" = "#5e3c99",
    "Conversational"  = "#cab2d6",
    "Punctuation"     = "#333333"
  )
)

color_breaks <- seq(-3, 3, length.out = 101)
gap_positions <- cumsum(sapply(theme_list_curated, length))[-length(theme_list_curated)]

# --- Step 4: Generate Heatmap ---
pdf(here::here("figures", "LIWC_Heatmap_Clustering.pdf"), width = 7, height = 5)

heatmap_liwc <- pheatmap(
  data_scaled,
  cluster_rows         = TRUE,
  cluster_cols         = FALSE,
  scale                = "none",
  color                = viridis(100, option = "D"),
  breaks               = color_breaks,
  border_color         = NA,
  show_rownames        = FALSE,
  show_colnames        = FALSE,
  gaps_col             = gap_positions,
  annotation_col       = annotation_col,
  annotation_colors    = theme_colors,
  main                 = "Linguistic Profiles of Participants",
  annotation_names_col = FALSE,
  legend               = TRUE,
  legend_breaks        = seq(-3, 3, by = 1),
  legend_labels        = c("-3 SD", "-2", "-1", "0", "1", "2", "3 SD"),
  silent               = TRUE # run silently
)
heatmap_liwc
dev.off()
```

The heatmap displays standardized (z-scored) scores for LIWC-22 linguistic variables. Each row represents a single participant, and each column represents a linguistic variable. Participants are clustered based on the similarity of their linguistic profiles, with the resulting dendrogram shown on the left. Columns are organized by their broader linguistic theme, as indicated by the top annotation bar. The cell color corresponds to the z-score, where warmer colors (e.g., yellow) indicate higher-than-average usage and cooler colors (e.g., purple) indicate lower-than-average usage of a specific linguistic feature.

## Baseline Models

This section corresponds to the baseline models reported in the Results text.

```{r baseline-models}
# ----------------------------
# T3: Internalizing - Simple Linear Model (for R-squared)
# ----------------------------

model.base.1 <- lm(
  YSR_Internalizing_Total.T3 ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1,
  data = df.liwc_imputed
)
# summary(model.base.1) # R-squared results reported in text

# ----------------------------
# T3: Internalizing - CV Linear Model: Demographics + SumSev
# ----------------------------

set.seed(12) # for reproducibility
baseline_formula_sumsev <- YSR_Internalizing_Total.T3 ~
  demo_Race.T1 +
  demo_Sex.T1 +
  demo_Age.T1 +
  demo_Parent_Edu.T1 +
  demo_INR.T1 +
  TESI_obj_sumsev.T1

n_folds <- 5
folds   <- sample(rep(1:n_folds, length.out = nrow(df.liwc_imputed)))

out_of_sample_predictions_sumsev <- rep(NA, nrow(df.liwc_imputed))

for (k in 1:n_folds) {
  train_idx <- which(folds != k)
  test_idx  <- which(folds == k)

  df_train <- df.liwc_imputed[train_idx, ]
  df_test  <- df.liwc_imputed[test_idx, ]

  model <- lm(baseline_formula_sumsev, data = df_train)
  predictions <- predict(model, newdata = df_test)
  out_of_sample_predictions_sumsev[test_idx] <- predictions
}

actual_values <- df.liwc_imputed$YSR_Internalizing_Total.T3
r_squared_sumsev <- 1 - sum((actual_values - out_of_sample_predictions_sumsev)^2) / 
                          sum((actual_values - mean(actual_values))^2)
# r_squared_sumsev value (0.070) reported in text and used in figure


# ----------------------------
# T3: Internalizing - CV Linear Model: Demographics ONLY
# ----------------------------

set.seed(1234) # for reproducibility
baseline_formula_demo <- YSR_Internalizing_Total.T3 ~
  demo_Race.T1 +
  demo_Sex.T1 +
  demo_Age.T1 +
  demo_Parent_Edu.T1 +
  demo_INR.T1

folds <- sample(rep(1:n_folds, length.out = nrow(df.liwc_imputed)))
out_of_sample_predictions_demo <- rep(NA, nrow(df.liwc_imputed))

for (k in 1:n_folds) {
  train_idx <- which(folds != k)
  test_idx  <- which(folds == k)

  df_train <- df.liwc_imputed[train_idx, ]
  df_test  <- df.liwc_imputed[test_idx, ]

  model <- lm(baseline_formula_demo, data = df_train)
  predictions <- predict(model, newdata = df_test)
  out_of_sample_predictions_demo[test_idx] <- predictions
}

r_squared_demo <- 1 - sum((actual_values - out_of_sample_predictions_demo)^2) / 
                      sum((actual_values - mean(actual_values))^2)
# r_squared_demo value (0.0248) used in figure


# ----------------------------
# T3: Diagnosis - Logistic Regression with Demographics + SumSev
# ----------------------------

df_with_dx <- merge(
  df.liwc_imputed,
  df.dx[, c(
    "ELS_ID", "T1_T4_any_intDx", "T2_T4_any_intDx",
    "T1_TA_any_intDx", "T2_TA_any_intDx", "T3_T4_any_intDx", "T3_TA_any_intDx"
  )],
  by = "ELS_ID",
  all.x = TRUE
)

model.logistic.base <- glm(
  T3_T4_any_intDx ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1,
  data = df_with_dx,
  family = binomial
)
# summary(model.logistic.base) # B=1.119, p=.012 for Sex reported in text
```


## Elastic Net

```{r elastic-net-cv}
# ----------------------------
# Run Elastic Net Model with 5-fold Nested CV
# ----------------------------

set.seed(123)
y <- df.liwc_imputed$YSR_Internalizing_Total.T3 # DV
X <- as.matrix(df.liwc_imputed[, c(2:6, 22:ncol(df.liwc_imputed))])
X <- matrix(as.numeric(X), nrow = nrow(X), dimnames = dimnames(X))
X <- scale(X)

n_outer_folds <- 5
folds <- sample(rep(1:n_outer_folds, length.out = length(y)))
alpha_grid <- seq(0, 1, by = 0.05)

cv_mse         <- c()
cv_rmse        <- c()
cv_mae         <- c()
cv_r_squared   <- c()
cv_predictions <- rep(NA, length(y))

best_alpha_folds  <- c()
best_lambda_folds <- c()

# outer CV loop
for (k in 1:n_outer_folds) {
  train_idx <- which(folds != k)
  test_idx  <- which(folds == k)

  X_train <- X[train_idx, ]
  y_train <- y[train_idx]
  X_test  <- X[test_idx, ]
  y_test  <- y[test_idx]

  # inner loop: tune alpha and lambda
  inner_cv_results <- data.frame()

  for (alpha_val in alpha_grid) {
    cv_model <- cv.glmnet(X_train, y_train, alpha = alpha_val, nfolds = 5)
    inner_cv_results <- rbind(
      inner_cv_results,
      data.frame(
        alpha = alpha_val,
        lambda_min = cv_model$lambda.min,
        cv_mse = min(cv_model$cvm)
      )
    )
  }

  # select best alpha/lambda from inner CV
  best_idx     <- which.min(inner_cv_results$cv_mse)
  best_alpha   <- inner_cv_results$alpha[best_idx]
  best_lambda  <- inner_cv_results$lambda_min[best_idx]

  best_alpha_folds  <- c(best_alpha_folds, best_alpha)
  best_lambda_folds <- c(best_lambda_folds, best_lambda)

  # fit model on outer train data
  final_model <- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda)

  # predict on outer test set
  y_pred <- predict(final_model, X_test, s = best_lambda)
  cv_predictions[test_idx] <- y_pred

  # compute performance
  mse       <- mean((y_test - y_pred)^2)
  rmse      <- sqrt(mse)
  mae       <- mean(abs(y_test - y_pred))
  r_squared <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

  cv_mse         <- c(cv_mse, mse)
  cv_rmse        <- c(cv_rmse, rmse)
  cv_mae         <- c(cv_mae, mae)
  cv_r_squared   <- c(cv_r_squared, r_squared)
}

# aggregate performance metrics
global_r_squared <- 1 - sum((y - cv_predictions)^2) / sum((y - mean(y))^2)
# global_r_squared (0.162) reported in text
```

```{r elastic-net-importance-plot}
# ----------------------------
# Model trained on full data for feature importance
# ----------------------------

set.seed(123)

# This DV should likely be:
y <- df.liwc_imputed$YSR_Internalizing_Total.T3 # DV
X <- as.matrix(df.liwc_imputed[, c(2:6, 22:ncol(df.liwc_imputed))])
X <- matrix(as.numeric(X), nrow = nrow(X), dimnames = dimnames(X))
X <- scale(X)

alpha_grid <- seq(0, 1, by = 0.05)
cv_results <- data.frame()

for (alpha_val in alpha_grid) {
  cv_model <- cv.glmnet(X, y, alpha = alpha_val, nfolds = 5)
  cv_results <- rbind(
    cv_results,
    data.frame(
      alpha = alpha_val,
      lambda_min = cv_model$lambda.min,
      cv_mse = min(cv_model$cvm)
    )
  )
}

best_idx     <- which.min(cv_results$cv_mse)
best_alpha   <- cv_results$alpha[best_idx]
best_lambda  <- cv_results$lambda_min[best_idx]

# refit final model on full dataset
final_model <- glmnet(X, y, alpha = best_alpha, lambda = best_lambda)

# ----------------------------
# Plotting: Top Predictors
# ----------------------------

coefficients <- as.data.frame(as.matrix(coef(final_model)))
coefficients$Predictors <- rownames(coefficients)
names(coefficients)[1]  <- "Coefficient"
coefficients <- coefficients[-1, , drop = FALSE] # remove intercept

coefficients <- coefficients %>%
  filter(Coefficient != 0) %>%
  slice_max(order_by = abs(Coefficient), n = 15) %>%
  mutate(Abs_Coefficient = abs(Coefficient)) %>%
  arrange(Abs_Coefficient)

# save plot
pdf(here::here("figures", "LIWC_EN_Predictors.pdf"), width = 7, height = 5)

ggplot(coefficients, aes(
  x = reorder(Predictors, Abs_Coefficient),
  y = Coefficient,
  fill = Coefficient > 0
)) +
  geom_bar(stat = "identity", color = "black") +
  scale_fill_manual(
    values = c("TRUE" = "#21908CFF", "FALSE" = "#FDE725FF"),
    labels = c("FALSE" = "Risk Factor", "TRUE" = "Protective"),
    name = "Association with Outcome"
  ) +
  coord_flip() +
  labs(
    title = "Key Predictors of Mental Health Problems",
    subtitle = "Standardized Coefficients from Elastic Net Model",
    x = "Predictor",
    y = "Standardized Coefficient"
  ) +
  theme_classic() +
  theme(
    axis.text.x = element_text(hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
    plot.subtitle = element_text(hjust = 0.5, face = "italic", size = 10)
  )

dev.off()


# ----------------------------
# Plotting: Predicted vs Observed Scatterplot
# ----------------------------

plot_data <- data.frame(Actual = y, Predicted = cv_predictions)
r_squared_val <- 1 - sum((plot_data$Actual - plot_data$Predicted)^2) / 
                       sum((plot_data$Actual - mean(plot_data$Actual))^2)
r2_label <- paste0("italic(R)^2 == ", round(r_squared_val, 3))

axis_min <- min(c(plot_data$Actual, plot_data$Predicted), na.rm = TRUE)
axis_max <- max(c(plot_data$Actual, plot_data$Predicted), na.rm = TRUE)

ggplot(plot_data, aes(x = Predicted, y = Actual)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  geom_point(color = viridis(1, option = "D", begin = 0.35), alpha = 0.6) +
  geom_smooth(method = "lm", color = "#FDE725FE", se = TRUE, fill = "gray80") +
  annotate(
    "text",
    x = axis_min + (axis_max - axis_min) * 0.05,
    y = axis_max,
    label = r2_label,
    hjust = 0,
    vjust = 1,
    parse = TRUE,
    size = 5
  ) +
  labs(
    title = "Model Performance on Held-Out Data",
    subtitle = "Predicted vs. Actual YSR Internalizing Scores",
    x = "Predicted Score (Out-of-Sample)",
    y = "Observed Score"
  ) +
  coord_fixed(ratio = 1, xlim = c(axis_min, axis_max), ylim = c(axis_min, axis_max)) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray30")
  )

ggsave(
  here::here("figures", "liwc_en_predicted_vs_observed.pdf"),
  width = 6, height = 6, units = "in"
)
```


## Variance Explained Across Models Figure

```{r variance-explained-plot, fig.height=2, fig.width=8}
# use r-squared values from baseline models
r2_demographics_only   <- r_squared_demo     # 0.0248 from CV model
r2_demographics_sumsev <- r_squared_sumsev   # 0.0702 from CV model
r2_full_model_liwc     <- global_r_squared # 0.16192 from EN CV model

# calculate incremental r-squared
incremental_demographics <- r2_demographics_only
incremental_sumsev       <- r2_demographics_sumsev - r2_demographics_only
incremental_liwc         <- r2_full_model_liwc - r2_demographics_sumsev
incremental_sumsev       <- max(0, incremental_sumsev)
incremental_liwc         <- max(0, incremental_liwc)

# create a tidy dataframe
plot_data <- tibble(
  model_name = "Model",
  variance_explained = c(incremental_demographics, incremental_sumsev, incremental_liwc),
  predictor_block = factor(
    c("Demographics Only", "+ Cumulative Stress", "+ LIWC Features"),
    levels = rev(c("Demographics Only", "+ Cumulative Stress", "+ LIWC Features"))
  )
) %>%
  mutate(
    cumulative_r2 = cumsum(variance_explained),
    label_pos = cumulative_r2 - variance_explained / 2
  )

# define color palette
base_viridis_colors <- viridis(3, option = "D")
muted_viridis_colors <- desaturate(base_viridis_colors, amount = 0.1)
plot_colors <- c(
  "Demographics Only"   = muted_viridis_colors[1],
  "+ Cumulative Stress" = muted_viridis_colors[2],
  "+ LIWC Features"     = muted_viridis_colors[3]
)

# create the plot
ggplot(plot_data, aes(x = variance_explained, y = model_name, fill = predictor_block)) +
  geom_col(color = "black", size = 0.3) +
  geom_text(
    aes(
      x = label_pos,
      label = paste0(round(variance_explained * 100, 1), "%"),
      color = ifelse(predictor_block == "+ LIWC Features", "black", "white")
    ),
    fontface = "bold",
    size = 4,
    show.legend = FALSE
  ) +
  scale_color_manual(values = c("black" = "black", "white" = "white")) +
  annotate(
    "text",
    x = r2_full_model_liwc + 0.005,
    y = "Model",
    label = paste("Total RÂ² =", round(r2_full_model_liwc * 100, 1), "%"),
    hjust = 0,
    fontface = "bold",
    size = 4
  ) +
  labs(
    title = "Incremental Variance Explained in Internalizing Problems",
    x = expression(paste("Cross-validated ", R^2)),
    y = NULL,
    fill = "Predictor Block"
  ) +
  scale_fill_manual(
    values = plot_colors,
    breaks = c("Demographics Only", "+ Cumulative Stress", "+ LIWC Features")
  ) +
  scale_x_continuous(
    limits = c(0, 0.22),
    breaks = seq(0, 0.20, by = 0.05)
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", margin = margin(b = 15)),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.text.y = element_blank(),
    legend.position = "right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_line(color = "gray85", linetype = "dashed")
  )

ggsave(
  here::here("figures", "liwc_incremental_R2_plot.pdf"),
  width = 8, height = 2, units = "in"
)
```

## PCA of LIWC Features

```{r pca-analysis}
# ----------------------------
# Run PCA
# ----------------------------
df.liwc_pca <- df.liwc_imputed[, 23:ncol(df.liwc_imputed)]
rownames(df.liwc_pca) <- df.liwc_imputed$ELS_ID

pca_result <- prcomp(df.liwc_pca, center = TRUE, scale. = TRUE)

# ----------------------------
# Extract PC Scores
# ----------------------------
pc_scores_with_ids <- pca_result$x %>%
  as.data.frame() %>%
  dplyr::select(PC1, PC2) %>%
  rownames_to_column("ELS_ID") %>%
  mutate(ELS_ID = as.character(ELS_ID))

# merge with original df
df_final_with_pcs <- df.liwc_imputed %>%
  mutate(ELS_ID = as.character(ELS_ID)) %>%
  left_join(pc_scores_with_ids, by = "ELS_ID")
```

```{r pca-scree-plot, fig.width=7, fig.height=5}
# ----------------------------
# Scree Plot of PCs
# ----------------------------
eigenvalues <- pca_result$sdev^2
variance_explained <- eigenvalues / sum(eigenvalues)

scree_data <- tibble(
  Component = 1:length(eigenvalues),
  Eigenvalue = eigenvalues,
  Variance_Explained = variance_explained
) %>%
  filter(Component <= 10)

ggplot(scree_data, aes(x = Component, y = Eigenvalue)) +
  geom_line(color = "gray60", linewidth = 1) +
  geom_point(size = 4, color = viridis(1, option = "D", begin = 0.35)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red", alpha = 0.6) +
  annotate(
    "segment",
    x = 2, xend = 2,
    y = 0.5, yend = scree_data$Eigenvalue[2] - 2,
    arrow = arrow(length = unit(0.2, "cm")),
    color = "gray30"
  ) +
  annotate(
    "text",
    x = 2.2, y = 2.8,
    label = "Elbow indicates\n2 components",
    hjust = 0,
    color = "gray30",
    fontface = "italic",
    size = 3.5
  ) +
  labs(
    title = "Scree Plot of Principal Components",
    subtitle = "Elbow at the second component suggests a two-factor solution",
    x = "Principal Component Number",
    y = "Eigenvalue (Variance Explained)"
  ) +
  scale_x_continuous(breaks = 1:10) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray30", size = 11),
    panel.grid.minor = element_blank()
  )

ggsave(
  here::here("figures", "liwc_pca_scree_plot.pdf"),
  width = 7, height = 5, units = "in"
)
```

```{r pca-biplot, fig.width=7, fig.height=5}
# ----------------------------
# BiPlot of PC1 and PC2
# ----------------------------
loadings <- as.data.frame(pca_result$rotation[, 1:2])
loadings$Feature <- rownames(loadings)

loadings <- loadings %>%
  mutate(loading_magnitude = sqrt(PC1^2 + PC2^2)) %>%
  mutate(Quadrant = case_when(
    PC1 > 0 & PC2 > 0 ~ "Top-Right",
    PC1 > 0 & PC2 < 0 ~ "Bottom-Right",
    PC1 < 0 & PC2 < 0 ~ "Bottom-Left",
    PC1 < 0 & PC2 > 0 ~ "Top-Left",
    TRUE ~ "Axis"
  ))

# select top 15 features
top_features <- loadings %>%
  arrange(desc(loading_magnitude)) %>%
  head(15) %>%
  mutate(Feature_Label = case_when(
    Feature == "socrefs" ~ "Social Refs",
    Feature == "cogproc" ~ "Cog. Processes",
    Feature == "auxverb" ~ "Aux. Verbs",
    TRUE ~ Feature
  )) %>%
  mutate( # manual nudge values
    nudge_x_val = 0,
    nudge_y_val = 0,
    nudge_x_val = ifelse(Feature_Label == "Cognition", -0.1, nudge_x_val),
    nudge_y_val = ifelse(Feature_Label == "Cognition", 0.2, nudge_y_val),
    nudge_x_val = ifelse(Feature_Label == "Period", 0.0, nudge_x_val),
    nudge_y_val = ifelse(Feature_Label == "Period", 1.0, nudge_y_val),
    nudge_x_val = ifelse(Feature_Label == "allure", 0.0, nudge_x_val),
    nudge_y_val = ifelse(Feature_Label == "allure", 1.95, nudge_y_val),
    nudge_x_val = ifelse(Feature_Label == "AllPunc", -0.1, nudge_x_val),
    nudge_y_val = ifelse(Feature_Label == "AllPunc", -1.4, nudge_y_val),
    nudge_x_val = ifelse(Feature_Label == "Analytic", -1.0, nudge_x_val),
    nudge_y_val = ifelse(Feature_Label == "Analytic", -1.0, nudge_y_val),
    nudge_x_val = ifelse(Feature_Label == "Drives", 0.1, nudge_x_val),
    nudge_y_val = ifelse(Feature_Label == "negate", -1.0, nudge_y_val)
  )

scores <- as.data.frame(pca_result$x[, 1:2])

ggplot() +
  geom_point(data = scores, aes(x = PC1, y = PC2), alpha = 0.2, color = "gray50") +
  geom_segment(
    data = top_features,
    aes(x = 0, y = 0, xend = PC1 * 20, yend = PC2 * 20, color = Quadrant),
    arrow = arrow(length = unit(0.2, "cm")),
    linewidth = 0.8
  ) +
  geom_text_repel(
    data = top_features,
    nudge_x = top_features$nudge_x_val,
    nudge_y = top_features$nudge_y_val,
    aes(x = PC1 * 20, y = PC2 * 20, label = Feature_Label, color = Quadrant),
    size = 4, fontface = "bold", box.padding = 0.8, max.overlaps = Inf
  ) +
  scale_color_manual(values = c(
    "Top-Right" = "#D62728", "Bottom-Right" = "#1F77B4",
    "Bottom-Left" = "#2CA02C", "Top-Left" = "#FF7F0E"
  )) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray30") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray30") +
  labs(
    title = "PCA Biplot of Linguistic Features",
    subtitle = "Displaying top 15 most influential features",
    x = paste0("PC1 (20% Variance)\n(Structured \u2194 Dynamic/Social)"),
    y = paste0("PC2 (6.8% Variance)\n(Cognitive Processing \u2194 Motivational)")
  ) +
  coord_cartesian(xlim = c(-4, 4), ylim = c(-5, 5)) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray30", size = 11),
    axis.title = element_text(size = 12),
    legend.position = "none"
  )

ggsave(
  here::here("figures", "liwc_pca_biPlot.pdf"),
  width = 7, height = 5, units = "in"
)
```


```{r pca-models-and-plots}
# ----------------------------
# Model YSR ~ PCs
# ----------------------------
model.pc <- lm(
  YSR_Internalizing_Total.T3 ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1 +
    PC1 +
    PC2,
  data = df_final_with_pcs
)
# summary(model.pc) # p = .004 for PC1, reported in text

# model comparison
model.pc_without <- lm(
  YSR_Internalizing_Total.T3 ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1,
  data = df_final_with_pcs
)

model.pc_with <- lm(
  YSR_Internalizing_Total.T3 ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1 +
    PC1,
  data = df_final_with_pcs
)
# anova(model.pc_without, model.pc_with) # F(1, 192) = 7.984, p = .005, reported in text

# ----------------------------
# YSR ~ PC1 Correlation Scatterplot
# ----------------------------
ggplot(df_final_with_pcs, aes(x = PC1, y = YSR_Internalizing_Total.T3)) +
  geom_point(color = viridis(1, option = "D", begin = 0.35), alpha = 0.6) +
  geom_smooth(method = "lm", color = "#FDE725FF", se = TRUE, fill = "gray80") +
  stat_cor(
    method = "pearson",
    aes(label = paste0(
      ..r.label..,
      "*', '*",
      ifelse(..p.. < 0.001,
        "italic(p)~`<`~.001",
        paste0("italic(p)~`=`~", sprintf("%.3f", ..p..))
      )
    )),
    label.x.npc = 0.05,
    label.y.npc = 0.95,
    hjust = 0,
    size = 5
  ) +
  labs(
    title = "Association Between Linguistic Style and Internalizing Problems",
    subtitle = "PC1 represents a dimension from structured to dynamic/social language",
    x = "PC1 Score (Dynamic/Social Language)",
    y = "YSR Internalizing Score"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray30", size = 10),
    panel.grid.minor = element_blank()
  )

ggsave(
  here::here("figures", "liwc_pca_scatter.pdf"),
  width = 6, height = 5, units = "in"
)

# ----------------------------
# Model Diagnosis ~ PCs (logistic)
# ----------------------------
df_final_with_pcs_dx <- merge(
  df_final_with_pcs,
  df.dx[, c(
    "ELS_ID", "T1_T4_any_intDx", "T2_T4_any_intDx",
    "T1_TA_any_intDx", "T2_TA_any_intDx", "T3_T4_any_intDx", "T3_TA_any_intDx"
  )],
  by = "ELS_ID",
  all.x = TRUE
)

model.logistic.pc <- glm(
  T3_T4_any_intDx ~
    demo_Race.T1 +
    demo_Sex.T1 +
    demo_Age.T1 +
    demo_Parent_Edu.T1 +
    demo_INR.T1 +
    TESI_obj_sumsev.T1 +
    PC1 +
    PC2,
  data = df_final_with_pcs_dx,
  family = binomial
)
# summary(model.logistic.pc) # OR and p-value reported in text
# exp(coef(model.logistic.pc))
# exp(confint(model.logistic.pc))

# ----------------------------
# Predicted Probability Plot
# ----------------------------
predicted_probs <- ggpredict(model.logistic.pc, terms = "PC1")

ggplot(predicted_probs) +
  geom_line(aes(x = x, y = predicted), color = "#21908CFF", linewidth = 1.2) +
  geom_ribbon(aes(x = x, ymin = conf.low, ymax = conf.high), alpha = 0.2, fill = "gray60") +
  geom_rug(
    data = df_final_with_pcs_dx,
    aes(x = PC1, color = as.factor(T3_T4_any_intDx)),
    sides = "b",
    alpha = 0.7,
    length = unit(0.04, "npc")
  ) +
  
  scale_color_manual(
    name = "Participant Status",
    values = c("1" = "red", "0" = "gray30"),
    labels = c("1" = "Diagnosis", "0" = "No Diagnosis")
  ) +
  labs(
    title = "Probability of Future Diagnosis by Linguistic Style",
    subtitle = "Predicted probability based on PC1, controlling for covariates",
    x = "PC1 Score (Structured \u2194 Dynamic/Social)",
    y = "Predicted Probability of Diagnosis"
  ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0, NA)) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray30", size = 10),
    panel.grid.minor = element_blank(),
    legend.position = "bottom"
  )

ggsave(
  here::here("figures", "liwc_pca_dx.pdf"),
  width = 7, height = 5, units = "in"
)
```